{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8584370b-3115-45ab-bce9-4de0a394309f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autograd import grad\n",
    "import autograd.numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d952e2f-4182-4eb1-b5c1-ba1f1d310d80",
   "metadata": {},
   "source": [
    "# Code from last lecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e67c094f-34a2-42af-8f5b-fb01630f1e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_lin = np.array([1., 2., 2., 3.])\n",
    "y_lin = np.array([1., 1., 2., 2.])\n",
    "\n",
    "\n",
    "x_quad = np.array([1., 2., 1.5, 3.])\n",
    "y_quad = np.array([1., 1., 0., 2.])\n",
    "\n",
    "def feature_map_linear(x):\n",
    "    # phi_i = x, 1\n",
    "    # so append a column of 1s, and we'll have 2 weights which will be m and b\n",
    "    return np.vstack([x, np.ones(len(x))]).T\n",
    "\n",
    "def feature_map_quadratic(x):\n",
    "    # phi_i = x**2, x, 1\n",
    "    # 3 weights -- a, b, c\n",
    "    return np.vstack([x**2, x, np.ones(len(x))]).T\n",
    "\n",
    "def gradient_descent(X, y, learning_rate=0.1, iters=1000):\n",
    "    # X is our feaure map, y is our target labels\n",
    "    m, n = X.shape\n",
    "    # n weights\n",
    "    w = np.zeros(n)\n",
    "\n",
    "    for _ in range(iters):\n",
    "        y_pred = X.dot(w)\n",
    "        # find gradient\n",
    "        gradient = X.T.dot(y_pred - y) / m\n",
    "        # update weights\n",
    "        w -= learning_rate * gradient\n",
    "    return w\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bb355a-5d45-4d7c-ae6d-c2651ee6d045",
   "metadata": {},
   "source": [
    "# Closed-form solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e31d72d8-c06c-4e37-af42-15ffdad58a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def closed_form_solution(X, y):\n",
    "    \n",
    "    phi = X\n",
    "    inverse_term = np.linalg.inv(phi.T.dot(phi))\n",
    "\n",
    "    w = inverse_term.dot(phi.T).dot(y)\n",
    "\n",
    "    return w\n",
    "    # Here's a more numerically stable way of doing it\n",
    "    # this will work on more general cases where the normal equation fails invertibility\n",
    "    # phi_pseudo_inverse = np.linalg.pinv(phi)\n",
    "    # w = phi_pseudo_inverse.dot(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16cf21fc-aad7-41da-b98d-cee173eafa17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient descent weights: [ 0.81818181 -2.64545452  2.63636361]\n",
      "closed-form solution weights: [ 0.81818182 -2.64545455  2.63636364]\n"
     ]
    }
   ],
   "source": [
    "X_quad = feature_map_quadratic(x_quad)\n",
    "w_quad_gd = gradient_descent(X_quad, y_quad, learning_rate=0.03, iters=100000)\n",
    "w_quad_cf = closed_form_solution(X_quad, y_quad)\n",
    "\n",
    "print(f\"gradient descent weights: {w_quad_gd}\")\n",
    "print(f\"closed-form solution weights: {w_quad_cf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cec5d1-25d2-45ac-8dfa-371a0793b6e5",
   "metadata": {},
   "source": [
    "# Multi-variate Regression\n",
    "- the main point here is that our feature map formulation will be different, but our GD and CF algorithms will not change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b241df6-8ad4-4aa7-9dbe-f7174185c189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 10)\n",
      "(300,)\n"
     ]
    }
   ],
   "source": [
    "X_stock = np.genfromtxt(\"./Data/stock_prediction_data.csv\", delimiter=\",\")\n",
    "y_stock = np.genfromtxt(\"./Data/stock_price.csv\", delimiter=\",\")\n",
    "\n",
    "print(X_stock.shape)\n",
    "print(y_stock.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5fbdfaf-6d40-4f12-9d58-8b1879db1cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly = PolynomialFeatures(2)\n",
    "phi_stock = poly.fit_transform(X_stock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "178773d0-6ec1-4dde-9502-2ac40975f8b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gradient descent is the same!\n",
    "w_quad_gd = gradient_descent(phi_stock, y_stock, learning_rate=0.01, iters=10000)\n",
    "w_quad_gd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a98ff9d-0d44-4d86-8f94-2c278404771c",
   "metadata": {},
   "source": [
    "## We got a bunch of nans, this is normal when we do things ourselves\n",
    "- usually means we divided by 0, exploded our gradients into infinity, or just generally did math thats too large to handle\n",
    "- easiest \"solution\" is to throw different standardization and boundaries until we find the problem and neutralize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4963cea8-7417-47b8-986a-5bc5e9518deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zmuv_norm(X):\n",
    "    # zero mean, unit variance standardization\n",
    "    mean = np.mean(X, axis=0)\n",
    "    std = np.std(X, axis=0)\n",
    "    std[std == 0] = 1\n",
    "    \n",
    "    return (X - mean) / std\n",
    "\n",
    "def clip_gradients(gradients, threshold=1.0):\n",
    "    # Clip gradients to help with the exploding gradient problem.\n",
    "    \n",
    "    gradient_norm = np.linalg.norm(gradients)\n",
    "    if gradient_norm > threshold:\n",
    "        gradients = gradients * threshold / gradient_norm\n",
    "    return gradients\n",
    "\n",
    "def more_stable_gradient_descent(X, y, learning_rate=0.01, iters=1000, gradient_clipping_threshold=1.0):\n",
    "    X = zmuv_norm(X)\n",
    "    \n",
    "    m, n = X.shape\n",
    "    w = np.zeros(n)\n",
    "\n",
    "    for _ in range(iters):\n",
    "        y_pred = X.dot(w)\n",
    "        gradients = X.T.dot(y_pred - y) / m\n",
    "\n",
    "        # Clip gradients to prevent large gradients\n",
    "        gradients = clip_gradients(gradients, threshold=gradient_clipping_threshold)\n",
    "\n",
    "        w -= learning_rate * gradients\n",
    "\n",
    "    return w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "388adb72-5a74-4288-852a-209c6aba6aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_quad_gd = more_stable_gradient_descent(phi_stock, y_stock, learning_rate=0.01, iters=10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ebe93350-23c9-4020-a93c-dd9376e86b16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        , -0.33130815,  3.10814204, -0.41436695, -0.14860673,\n",
       "        0.6767484 ,  1.96620149,  3.00633895,  1.6425693 ,  0.32395045,\n",
       "        0.96801387,  0.07091873,  0.06556882,  0.05192631, -0.00715662,\n",
       "        0.0762644 , -0.00840449,  0.05147481,  0.12233287,  0.08920206,\n",
       "        0.00794382,  0.25956858,  0.11256542,  0.13383122,  0.08147415,\n",
       "        0.11328582,  0.15645011,  0.18643335,  0.12266274, -0.03618363,\n",
       "       -0.0031777 ,  0.01250698, -0.05627823,  0.10286842,  0.1239688 ,\n",
       "        0.22816204,  0.05841332,  0.0049547 , -0.05677831, -0.00545771,\n",
       "        0.09131472,  0.02852551,  0.12119848,  0.02153226, -0.09568235,\n",
       "        0.00755069,  0.2765269 ,  0.04779805,  0.06517629, -0.01574148,\n",
       "       -0.04123438,  0.35957758,  0.11676249,  0.14640058,  0.17755373,\n",
       "        0.0521094 ,  0.35999005,  0.22096603,  0.14679412,  0.1012213 ,\n",
       "        0.53018099,  0.0704341 ,  0.06523659,  0.18189582,  0.02407394,\n",
       "       -0.01506365])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_quad_gd"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
